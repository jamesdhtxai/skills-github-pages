title: "A09 Ethical and Societal Implications of AI - Case Study"
date: 2025-03-27


Ethical Use of AI at Work: Navigating a Personal and Professional Balance

AI tools like ChatGPT are rapidly changing the way we work. I use ChatGPT regularly—sometimes daily. It helps me understand complex topics faster, draft cleaner work emails, and prepare for discussions with more clarity. But while it’s made me more productive, it’s also raised questions I can’t ignore.

One of the most helpful frameworks I’ve come across is from Dr. Bruce Weinstein in his Forbes article where he lays out five principles for ethical AI use: Do No Harm, Make Things Better, Respect Others, Be Fair, and Care (Weinstein, 2023). These aren’t just corporate buzzwords—they’re guideposts I try to follow whenever I open ChatGPT. For example, I never just copy-paste its answers. I cross-check facts, run my own searches, and use its responses as a jumping-off point rather than the final word. That’s part of what Weinstein calls “Prevent Harm,” which is about doing your homework before spreading or acting on AI-generated info.

Even so, there’s a nagging feeling that I’m cheating the system. Like I’ve got this secret weapon that most people don’t talk about, even though they’re probably using it too. I’ll bring it up with coworkers I trust, but not with everyone—some might judge me, or worse, report it. One specific example that sticks with me is how I record in-person meetings. Zoom makes it easy by showing a recording banner, but when I’m in a room with no tech, I’ll use a voice recorder and then run that through a transcription tool to help recap later. It’s super effective, but it also feels ethically fuzzy—mostly because there’s no easy way to notify people unless I awkwardly announce it every time. That ties into Weinstein’s point about “Respecting Others,” especially when it comes to transparency and confidentiality.

Most articles talk about how companies are using AI to monitor their employees, but the Financial Times piece “Bosses struggle to police workers’ use of AI” flips the script (Williams & Nilsson, 2024). It looks at how employees are using AI—often in secret—to keep up with increasing workloads or expectations. That really hit home for me. The article points out that when companies issue vague or strict rules, people just keep using AI under the radar. That doesn’t solve anything. What’s needed are clearer policies that separate smart, ethical use from actual misuse.
That’s where I see myself. I don’t use ChatGPT to avoid thinking—I use it to push my thinking further. It helps me ask better questions, structure my ideas, and explore topics I might not have considered. But the hesitation to be open about it? That’s real. And that’s why Weinstein’s push for honesty—owning how you use AI and making sure it’s accurate—is so important. It’s not about hiding the tool; it’s about showing you’re using it with integrity.

As AI becomes a normal part of how we work, the question isn’t whether we use it, but how we do so responsibly. We need better workplace norms—ones that don’t punish ethical use, but instead encourage people to be thoughtful and open. Because the real power of AI isn’t in what it can do alone—it’s in what we do with it, responsibly and transparently.

References: Weinstein, B. (2023). Why Smart Leaders Use ChatGPT Ethically—And How They Do It. Forbes. https://www.forbes.com/sites/bruceweinstein/2023/02/24/why-smart-leaders-use-chatgpt-ethically-and-how-they-do-it/
Williams, A., & Nilsson, P. (2024). Bosses struggle to police workers’ use of AI. Financial Times. https://www.ft.com/content/cd08b45d-12dc-447e-bd59-1c366a7e6396

